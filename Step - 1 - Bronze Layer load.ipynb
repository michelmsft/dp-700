{"cells":[{"cell_type":"markdown","source":["\n","### **HIGH-LEVEL PURPOSE**\n","\n","This script bulk-loads CSV files from a \"Raw\" landing zone into MANAGED Delta tables\n","in the current Lakehouse database. It:\n","1) Reads CSVs for two groups of datasets: reference and operations.\n","2) Uses wildcard (glob) paths to sweep all countries/folders at once.\n","3) Derives a 'Source' column (country) from each file's path using regex on input_file_name().\n","4) Writes the results directly as MANAGED Delta tables via saveAsTable(), partitioned by 'Source'.\n","5) Overwrites tables each run (idempotent full refresh)."],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"fb221a8d-38ee-4277-b231-f6d743da8693"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","import re\n","\n","# Root folder where raw CSVs land, organized by country folders (e.g., Files/Raw/USA/...).\n","raw_root = \"Files/Raw\"\n","\n","# Current database (Lakehouse DB) where MANAGED Delta tables will be created/overwritten.\n","db = spark.catalog.currentDatabase()\n","\n","# Lists of expected CSV filenames. The script will build one table per filename (minus \".csv\").\n","reference_files = [\n","    \"Customers.csv\", \"Employees.csv\", \"Machines.csv\",\n","    \"Mills.csv\", \"Products.csv\", \"Warehouses.csv\"\n","]\n","\n","operations_files = [\n","    \"InventorySnapshots.csv\", \"MachineDowntime.csv\", \"MachineSensors.csv\",\n","    \"Orders.csv\", \"PlannedProductions.csv\", \"Shipments.csv\", \"StockMovements.csv\"\n","]"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":7,"statement_ids":[7],"state":"finished","livy_statement_state":"available","session_id":"a2845f01-1246-40d2-beef-03623fd98642","normalized_state":"finished","queued_time":"2025-08-13T05:50:06.3343215Z","session_start_time":null,"execution_start_time":"2025-08-13T05:50:06.3354905Z","execution_finish_time":"2025-08-13T05:50:06.6349691Z","parent_msg_id":"5e745cf4-b303-49e4-b4cd-00573fe863d7"},"text/plain":"StatementMeta(, a2845f01-1246-40d2-beef-03623fd98642, 7, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"f34e6b72-7e42-4600-9959-44a683249b87"},{"cell_type":"markdown","source":["\n","#### **HOW 'Source' (country) IS EXTRACTED**\n","\n","Example path: Files/Raw/Brazil/reference/Customers.csv\n","We want to capture \"Brazil\" regardless of which subfolder comes next.\n","   - re.escape(raw_root) safely treats any special chars in raw_root as literals.\n","   - r\"/([^/]+)/\" captures the segment immediately after raw_root (the country).\n","\n","Concretely, for:\n","```\n","   input_file_name = \".../Files/Raw/Brazil/reference/Customers.csv\"\n","  regex group(1)   = \"Brazil\"\n","```"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"0d1a6fcd-9805-4117-8ac5-1dde321d145b"},{"cell_type":"code","source":["from pyspark.sql import functions as F\n","import re\n","\n","\n","country_regex = re.escape(raw_root) + r\"/([^/]+)/\"\n","\n","def load_to_managed(files, folder_structure):\n","    \"\"\"\n","    For each CSV filename in `files`:\n","      - Build a glob path using `folder_structure` (e.g., Files/Raw/*/reference/{file})\n","      - Read all matching CSVs across countries into a single DataFrame.\n","      - Add 'Source' (country) by regex against input_file_name().\n","      - Save as a MANAGED Delta table in `db`, partitioned by 'Source', overwriting any prior table.\n","    \"\"\"\n","    for fn in files:\n","        # Derive a table name by dropping \".csv\".\n","        table = fn[:-4]\n","        table_qual = f\"{db}.{table}\"\n","\n","        # Turn a pattern like \"Files/Raw/*/reference/{file}\" into a concrete glob.\n","        #   e.g., \"Files/Raw/*/reference/Customers.csv\"\n","        src_glob = folder_structure.format(file=fn)\n","\n","        try:\n","            # ---------------------------\n","            # LOAD ALL MATCHING FILES\n","            # ---------------------------\n","            # - .option(\"header\",\"true\"): Use the first row as column names.\n","            # - .option(\"inferSchema\",\"true\"): Let Spark guess column types from the data (fast setup, but\n","            #   consider specifying an explicit schema for stability/production).\n","            # - .load(src_glob): Reads every file that matches the wildcard pattern.\n","            # - .withColumn(\"Source\", ...): Pull the country from the file path.\n","            df = (\n","                spark.read.format(\"csv\")\n","                    .option(\"header\", \"true\")\n","                    .option(\"inferSchema\", \"true\")\n","                    .load(src_glob)\n","                    .withColumn(\"Source\", F.regexp_extract(F.input_file_name(), country_regex, 1))\n","            )\n","\n","            # ---------------------------\n","            # WRITE AS MANAGED DELTA TABLE\n","            # ---------------------------\n","            # saveAsTable(table_qual):\n","            #   - Creates (or replaces) a MANAGED table stored in the Lakehouse's managed location.\n","            #   - We use mode(\"overwrite\") + option(\"overwriteSchema\",\"true\") for idempotent, full-refresh loads\n","            #     that also adapt to schema changes.\n","            # partitionBy(\"Source\"):\n","            #   - Physically partitions table data by the country. This speeds up queries that filter on Source,\n","            #     and makes it easy to manage per-country data.\n","            (df.write.format(\"delta\")\n","               .mode(\"overwrite\")\n","               .option(\"overwriteSchema\", \"true\")\n","               .partitionBy(\"Source\")\n","               .saveAsTable(table_qual))\n","\n","            print(f\"‚úÖ {table_qual}: loaded as managed Delta table\")\n","        except Exception as e:\n","            # Any read/write issue (e.g., missing files, malformed CSV) is reported,\n","            # and the loop continues to the next table.\n","            print(f\"‚ö†Ô∏è {table}: skipped ‚Äî {e}\")\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"a2845f01-1246-40d2-beef-03623fd98642","normalized_state":"finished","queued_time":"2025-08-13T05:50:06.4042351Z","session_start_time":null,"execution_start_time":"2025-08-13T05:50:06.6371134Z","execution_finish_time":"2025-08-13T05:50:06.9485981Z","parent_msg_id":"4e3abc8d-5f71-4991-b9f8-94bdd949f228"},"text/plain":"StatementMeta(, a2845f01-1246-40d2-beef-03623fd98642, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"dcb39ac1-c1f0-4a77-b42d-a4a9e5f8db16"},{"cell_type":"markdown","source":["#### **INVOCATION / FOLDER PATTERNS**"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"2d04f4fc-19a4-4b67-8e97-969e42fd123c"},{"cell_type":"code","source":["\n","# Reference data lives under: <country>/reference/<file>.csv\n","#   e.g., Files/Raw/USA/reference/Customers.csv\n","# The glob \"*\": sweep all countries without enumerating them.\n","load_to_managed(reference_files,  f\"{raw_root}/*/reference/{{file}}\")\n","\n","# Operational data lives under: <country>/<some_subfolder>/<file>.csv,\n","# often date-stamped or otherwise nested.\n","#   e.g., Files/Raw/USA/2025-08-01/Orders.csv\n","# The glob \"*/*\": any one-level subfolder beneath country.\n","load_to_managed(operations_files, f\"{raw_root}/*/*/{{file}}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"a2845f01-1246-40d2-beef-03623fd98642","normalized_state":"finished","queued_time":"2025-08-13T05:50:06.6073294Z","session_start_time":null,"execution_start_time":"2025-08-13T05:50:06.9505679Z","execution_finish_time":"2025-08-13T05:51:12.300962Z","parent_msg_id":"3889baa9-1c10-45f1-995a-e88266475f4b"},"text/plain":"StatementMeta(, a2845f01-1246-40d2-beef-03623fd98642, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ lakehouse_bz.Customers: loaded as managed Delta table\n‚úÖ lakehouse_bz.Employees: loaded as managed Delta table\n‚úÖ lakehouse_bz.Machines: loaded as managed Delta table\n‚úÖ lakehouse_bz.Mills: loaded as managed Delta table\n‚úÖ lakehouse_bz.Products: loaded as managed Delta table\n‚úÖ lakehouse_bz.Warehouses: loaded as managed Delta table\n‚úÖ lakehouse_bz.InventorySnapshots: loaded as managed Delta table\n‚úÖ lakehouse_bz.MachineDowntime: loaded as managed Delta table\n‚úÖ lakehouse_bz.MachineSensors: loaded as managed Delta table\n‚úÖ lakehouse_bz.Orders: loaded as managed Delta table\n‚úÖ lakehouse_bz.PlannedProductions: loaded as managed Delta table\n‚úÖ lakehouse_bz.Shipments: loaded as managed Delta table\n‚úÖ lakehouse_bz.StockMovements: loaded as managed Delta table\n"]}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e596a749-20c5-4c9d-b7b4-bf4e9a234034"},{"cell_type":"code","source":["from notebookutils import mssparkutils\n","\n","delete_tables = False  # <-- set True to delete the folders too\n","\n","if delete_tables:\n","    # same list you used when creating\n","    tables = [\n","        \"Customers\",\"Employees\",\"Machines\",\"Mills\",\"Products\",\"Warehouses\",\n","        \"InventorySnapshots\",\"MachineDowntime\",\"MachineSensors\",\n","        \"Orders\",\"PlannedProductions\",\"Shipments\",\"StockMovements\"\n","    ]\n","\n","    # Drop from the current Lakehouse database (what Fabric shows as the schema)\n","    db = spark.catalog.currentDatabase()\n","    print(f\"Dropping from database: {db}\")\n","\n","    for t in tables:\n","        spark.sql(f\"DROP TABLE IF EXISTS {db}.{t}\")\n","        print(f\"üóëÔ∏è  Dropped: {db}.{t}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":10,"statement_ids":[10],"state":"finished","livy_statement_state":"available","session_id":"a2845f01-1246-40d2-beef-03623fd98642","normalized_state":"finished","queued_time":"2025-08-13T05:50:06.817989Z","session_start_time":null,"execution_start_time":"2025-08-13T05:51:12.3035464Z","execution_finish_time":"2025-08-13T05:51:12.5868654Z","parent_msg_id":"8101f056-5c0d-423e-a54b-52d0a0b3ca5d"},"text/plain":"StatementMeta(, a2845f01-1246-40d2-beef-03623fd98642, 10, Finished, Available, Finished)"},"metadata":{}}],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"ef7aa4a6-bb00-4149-a558-e7e25db37fdd"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"bcabd2eb-40fe-465b-b9f6-518f41325eae","known_lakehouses":[{"id":"bcabd2eb-40fe-465b-b9f6-518f41325eae"}],"default_lakehouse_name":"Lakehouse_Bz","default_lakehouse_workspace_id":"399eee70-3af7-4903-a62d-6c5acb3ade34"}}},"nbformat":4,"nbformat_minor":5}