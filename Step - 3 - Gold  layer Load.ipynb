{"cells":[{"cell_type":"markdown","source":["#### **GOLD loader for lakehouse_sv  → lakehouse_gd**\n"," - Dimensions: DimDate, DimProduct, DimWarehouse, DimCustomer (opt), DimMill (opt), DimMachine (opt), DimEmployee (opt)\n"," - Facts: FactOrders, FactShipments, FactDailyFillRate, FactWarehouseDailyUtilization\n"," - Deterministic SurrogateKey = xxhash64 of natural keys\n"," - MERGE auto-adds missing columns; joins to DimDate use aliases to avoid ambiguity"],"metadata":{"nteract":{"transient":{"deleting":false}},"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"36a4f0b7-f313-43e0-acae-0c10c8cc4150"},{"cell_type":"code","source":["from pyspark.sql import functions as F, Window\n","from delta.tables import DeltaTable\n","\n","# -----------------------------\n","# CONFIG\n","# -----------------------------\n","SILVER_DB = \"lakehouse_sv\"\n","GOLD_DB   = \"lakehouse_gd\"\n","\n","# -----------------------------\n","# HELPERS\n","# -----------------------------\n","def texists(db, name):\n","    return spark.catalog.tableExists(f\"{db}.{name}\")\n","\n","def sk_hash(cols):\n","    \"\"\"Deterministic 64-bit surrogate key from natural key columns.\"\"\"\n","    exprs = [F.coalesce(F.col(c).cast(\"string\"), F.lit(\"\")) for c in cols]\n","    return F.abs(F.xxhash64(*exprs)).cast(\"long\")\n","\n","def _add_missing_columns(table_fqn, df):\n","    \"\"\"Add any columns present in df but missing in target table (types from df).\"\"\"\n","    tgt_cols = set(spark.table(table_fqn).columns)\n","    missing  = [f for f in df.schema.fields if f.name not in tgt_cols]\n","    for f in missing:\n","        sql_type = f.dataType.simpleString()  # e.g. 'string', 'int', 'decimal(10,2)', 'date'\n","        spark.sql(f\"ALTER TABLE {table_fqn} ADD COLUMNS ({f.name} {sql_type})\")\n","\n","def merge_upsert(table_fqn, df, natural_key_cols, partition_by=None):\n","    \"\"\"\n","    Create/merge managed Delta.\n","    - Creates table if missing.\n","    - If exists: auto-add new columns, then MERGE using only columns common to both.\n","    - SurrogateKey is provided by df (deterministic); safe to overwrite.\n","    \"\"\"\n","    if not spark.catalog.tableExists(table_fqn):\n","        w = (df.write.format(\"delta\").mode(\"overwrite\").option(\"overwriteSchema\", \"true\"))\n","        if partition_by:\n","            w = w.partitionBy(*partition_by)\n","        w.saveAsTable(table_fqn)\n","        print(f\"🆕 {table_fqn}: created\")\n","        return\n","\n","    _add_missing_columns(table_fqn, df)\n","\n","    dt   = DeltaTable.forName(spark, table_fqn)\n","    cond = \" AND \".join([f\"t.{c} = s.{c}\" for c in natural_key_cols])\n","\n","    target_cols = set(spark.table(table_fqn).columns)\n","    common_cols = [c for c in df.columns if c in target_cols]\n","\n","    set_insert = {c: f\"s.{c}\" for c in common_cols}\n","    set_update = {c: f\"s.{c}\" for c in common_cols}\n","\n","    (dt.alias(\"t\")\n","       .merge(df.alias(\"s\"), cond)\n","       .whenMatchedUpdate(set=set_update)\n","       .whenNotMatchedInsert(values=set_insert)\n","       .execute())\n","    print(f\"✅ {table_fqn}: merged on {natural_key_cols}\")\n","\n","# -----------------------------\n","# DIMENSIONS\n","# -----------------------------\n","# DimProduct\n","if texists(SILVER_DB, \"products\"):\n","    prod = (spark.table(f\"{SILVER_DB}.products\")\n","              .dropDuplicates([\"ProductID\",\"Source\"])\n","              .withColumn(\"PalletsToTon\", F.coalesce(F.col(\"PalletsToTon\").cast(\"double\"), F.lit(1.0)))\n","              .withColumn(\"SurrogateKey\", sk_hash([\"ProductID\",\"Source\"])))\n","    merge_upsert(f\"{GOLD_DB}.DimProduct\", prod,\n","                 natural_key_cols=[\"ProductID\",\"Source\"],\n","                 partition_by=[\"Source\"])\n","\n","# DimWarehouse\n","if texists(SILVER_DB, \"warehouses\"):\n","    wh = (spark.table(f\"{SILVER_DB}.warehouses\")\n","            .dropDuplicates([\"WarehouseID\",\"Source\"])\n","            .withColumn(\"CapacityPallets\", F.col(\"CapacityPallets\").cast(\"double\"))\n","            .withColumn(\"SurrogateKey\", sk_hash([\"WarehouseID\",\"Source\"])))\n","    merge_upsert(f\"{GOLD_DB}.DimWarehouse\", wh,\n","                 natural_key_cols=[\"WarehouseID\",\"Source\"],\n","                 partition_by=[\"Source\"])\n","\n","# DimCustomer (optional)\n","if texists(SILVER_DB, \"customers\"):\n","    cus = (spark.table(f\"{SILVER_DB}.customers\")\n","             .dropDuplicates([\"CustomerID\",\"Source\"])\n","             .withColumn(\"SurrogateKey\", sk_hash([\"CustomerID\",\"Source\"])))\n","    merge_upsert(f\"{GOLD_DB}.DimCustomer\", cus,\n","                 natural_key_cols=[\"CustomerID\",\"Source\"],\n","                 partition_by=[\"Source\"])\n","\n","# DimMill (optional)\n","if texists(SILVER_DB, \"mills\"):\n","    mill = (spark.table(f\"{SILVER_DB}.mills\")\n","              .dropDuplicates([\"MillID\",\"Source\"])\n","              .withColumn(\"SurrogateKey\", sk_hash([\"MillID\",\"Source\"])))\n","    merge_upsert(f\"{GOLD_DB}.DimMill\", mill,\n","                 natural_key_cols=[\"MillID\",\"Source\"],\n","                 partition_by=[\"Source\"])\n","\n","# DimMachine (optional)\n","if texists(SILVER_DB, \"machines\"):\n","    mac = (spark.table(f\"{SILVER_DB}.machines\")\n","             .dropDuplicates([\"MachineID\",\"Source\"])\n","             .withColumn(\"SurrogateKey\", sk_hash([\"MachineID\",\"Source\"])))\n","    merge_upsert(f\"{GOLD_DB}.DimMachine\", mac,\n","                 natural_key_cols=[\"MachineID\",\"Source\"],\n","                 partition_by=[\"Source\"])\n","\n","# DimEmployee (optional)\n","if texists(SILVER_DB, \"employees\"):\n","    emp = (spark.table(f\"{SILVER_DB}.employees\")\n","             .dropDuplicates([\"EmployeeID\",\"Source\"])\n","             .withColumn(\"SurrogateKey\", sk_hash([\"EmployeeID\",\"Source\"])))\n","    merge_upsert(f\"{GOLD_DB}.DimEmployee\", emp,\n","                 natural_key_cols=[\"EmployeeID\",\"Source\"],\n","                 partition_by=[\"Source\"])\n","\n","# DimDate (union of dates)\n","date_frames = []\n","if texists(SILVER_DB, \"orders\"):\n","    date_frames.append(spark.table(f\"{SILVER_DB}.orders\")\n","                           .select(F.col(\"OrderDate\").alias(\"date\"))\n","                           .where(F.col(\"OrderDate\").isNotNull()))\n","if texists(SILVER_DB, \"shipments\"):\n","    date_frames.append(spark.table(f\"{SILVER_DB}.shipments\")\n","                           .select(F.col(\"Date\").alias(\"date\"))\n","                           .where(F.col(\"Date\").isNotNull()))\n","if texists(SILVER_DB, \"stockmovements\"):\n","    date_frames.append(spark.table(f\"{SILVER_DB}.stockmovements\")\n","                           .select(F.col(\"Date\").alias(\"date\"))\n","                           .where(F.col(\"Date\").isNotNull()))\n","if texists(SILVER_DB, \"plannedproductions\"):\n","    date_frames.append(spark.table(f\"{SILVER_DB}.plannedproductions\")\n","                           .select(F.col(\"ProdDate\").alias(\"date\"))\n","                           .where(F.col(\"ProdDate\").isNotNull()))\n","if texists(SILVER_DB, \"machinedowntime\"):\n","    md = spark.table(f\"{SILVER_DB}.machinedowntime\")\n","    date_frames.append(md.select(F.to_date(\"StartTime\").alias(\"date\")).where(F.col(\"StartTime\").isNotNull()))\n","    date_frames.append(md.select(F.to_date(\"EndTime\").alias(\"date\")).where(F.col(\"EndTime\").isNotNull()))\n","if texists(SILVER_DB, \"machinesensors\"):\n","    date_frames.append(spark.table(f\"{SILVER_DB}.machinesensors\")\n","                           .select(F.to_date(\"Timestamp\").alias(\"date\"))\n","                           .where(F.col(\"Timestamp\").isNotNull()))\n","\n","if date_frames:\n","    dim_date = date_frames[0]\n","    for d in date_frames[1:]:\n","        dim_date = dim_date.unionByName(d, allowMissingColumns=True)\n","    dim_date = (dim_date.distinct()\n","                        .withColumn(\"date\", F.to_date(\"date\"))\n","                        .withColumn(\"Year\",     F.year(\"date\"))\n","                        .withColumn(\"Month\",    F.month(\"date\"))\n","                        .withColumn(\"Day\",      F.dayofmonth(\"date\"))\n","                        .withColumn(\"Quarter\",  F.quarter(\"date\"))\n","                        .withColumn(\"DateSK\",   (F.year(\"date\")*10000 + F.month(\"date\")*100 + F.dayofmonth(\"date\")).cast(\"int\"))\n","                        .withColumn(\"SurrogateKey\", F.col(\"DateSK\").cast(\"long\")))\n","    merge_upsert(f\"{GOLD_DB}.DimDate\", dim_date, natural_key_cols=[\"date\"])\n","\n","# -----------------------------\n","# DIM LOOKUPS FOR FACT JOINS (no ambiguity)\n","# -----------------------------\n","DimProductLK   = spark.table(f\"{GOLD_DB}.DimProduct\").select(\"ProductID\",\"Source\",\"PalletsToTon\",\"SurrogateKey\").withColumnRenamed(\"SurrogateKey\",\"ProductSK\") if texists(GOLD_DB,\"DimProduct\") else None\n","DimWarehouseLK = spark.table(f\"{GOLD_DB}.DimWarehouse\").select(\"WarehouseID\",\"Source\",\"SurrogateKey\").withColumnRenamed(\"SurrogateKey\",\"WarehouseSK\") if texists(GOLD_DB,\"DimWarehouse\") else None\n","DimCustomerLK  = spark.table(f\"{GOLD_DB}.DimCustomer\").select(\"CustomerID\",\"Source\",\"SurrogateKey\").withColumnRenamed(\"SurrogateKey\",\"CustomerSK\") if texists(GOLD_DB,\"DimCustomer\") else None\n","DimDateLK      = spark.table(f\"{GOLD_DB}.DimDate\").select(\"date\",\"SurrogateKey\").withColumnRenamed(\"SurrogateKey\",\"DateSK\") if texists(GOLD_DB,\"DimDate\") else None\n","\n","# -----------------------------\n","# FACTS\n","# -----------------------------\n","\n","# FactOrders\n","if texists(SILVER_DB, \"orders\") and DimProductLK is not None and DimDateLK is not None:\n","    o = spark.table(f\"{SILVER_DB}.orders\").alias(\"o\")\n","    p = DimProductLK.alias(\"p\")\n","    d = DimDateLK.alias(\"d\")\n","    c = DimCustomerLK.alias(\"c\") if DimCustomerLK is not None else None\n","\n","    o1 = o.join(p, on=[\"ProductID\",\"Source\"], how=\"left\") \\\n","          .withColumn(\"OrderDate_d\", F.to_date(F.col(\"o.OrderDate\"))) \\\n","          .join(d, F.col(\"OrderDate_d\") == F.col(\"d.date\"), how=\"left\")\n","\n","    ratio = F.when(F.col(\"p.PalletsToTon\") == 0, F.lit(1.0)).otherwise(F.col(\"p.PalletsToTon\"))\n","    o1 = (o1.withColumn(\"OrderedPallets\", F.col(\"o.QuantityPallets\").cast(\"double\"))\n","            .withColumn(\"OrderedTons\",    F.col(\"o.QuantityPallets\").cast(\"double\") / ratio))\n","\n","    if c is not None and \"CustomerID\" in o1.columns:\n","        o1 = o1.join(c, on=[\"CustomerID\",\"Source\"], how=\"left\")\n","\n","    o1 = (o1.withColumn(\"NaturalKey\", F.sha2(F.concat_ws(\"|\",\n","                     F.col(\"o.OrderID\").cast(\"string\"),\n","                     F.col(\"o.ProductID\").cast(\"string\"),\n","                     F.coalesce(F.col(\"o.Source\").cast(\"string\"), F.lit(\"\"))), 256))\n","            .withColumn(\"SurrogateKey\", sk_hash([\"NaturalKey\"])))\n","\n","    fact_orders = o1.select(\n","        \"SurrogateKey\",\"NaturalKey\",\n","        \"d.DateSK\",\"p.ProductSK\",\n","        *([\"c.CustomerSK\"] if c is not None and \"CustomerSK\" in o1.columns else []),\n","        F.col(\"o.Source\").alias(\"Source\"),\n","        \"OrderedPallets\",\"OrderedTons\"\n","    )\n","    merge_upsert(f\"{GOLD_DB}.FactOrders\", fact_orders,\n","                 natural_key_cols=[\"NaturalKey\"],\n","                 partition_by=[\"Source\"])\n","\n","# FactShipments\n","if texists(SILVER_DB, \"shipments\") and DimProductLK is not None and DimWarehouseLK is not None and DimDateLK is not None:\n","    s = spark.table(f\"{SILVER_DB}.shipments\").alias(\"s\")\n","    p = DimProductLK.alias(\"p\")\n","    w = DimWarehouseLK.alias(\"w\")\n","    d = DimDateLK.alias(\"d\")\n","\n","    s1 = s.join(p, on=[\"ProductID\",\"Source\"], how=\"left\") \\\n","          .join(w, on=[\"WarehouseID\",\"Source\"], how=\"left\") \\\n","          .withColumn(\"ShipDate\", F.to_date(F.col(\"s.Date\"))) \\\n","          .join(d, F.col(\"ShipDate\") == F.col(\"d.date\"), how=\"left\")\n","\n","    ratio = F.when(F.col(\"p.PalletsToTon\") == 0, F.lit(1.0)).otherwise(F.col(\"p.PalletsToTon\"))\n","    s1 = (s1.withColumn(\"ShippedPallets\", F.col(\"s.Pallets\").cast(\"double\"))\n","            .withColumn(\"ShippedTons\",    F.col(\"s.Pallets\").cast(\"double\") / ratio))\n","\n","    s1 = (s1.withColumn(\"NaturalKey\", F.sha2(F.concat_ws(\"|\",\n","                     F.col(\"s.ShipmentID\").cast(\"string\"),\n","                     F.col(\"s.ProductID\").cast(\"string\"),\n","                     F.col(\"s.WarehouseID\").cast(\"string\"),\n","                     F.coalesce(F.col(\"s.Source\").cast(\"string\"), F.lit(\"\"))), 256))\n","            .withColumn(\"SurrogateKey\", sk_hash([\"NaturalKey\"])))\n","\n","    fact_ship = s1.select(\n","        \"SurrogateKey\",\"NaturalKey\",\n","        \"d.DateSK\",\"p.ProductSK\",\"w.WarehouseSK\",\n","        F.col(\"s.Source\").alias(\"Source\"),\n","        \"ShippedPallets\",\"ShippedTons\"\n","    )\n","    merge_upsert(f\"{GOLD_DB}.FactShipments\", fact_ship,\n","                 natural_key_cols=[\"NaturalKey\"],\n","                 partition_by=[\"Source\"])\n","\n","# FactDailyFillRate (date grain)\n","if texists(SILVER_DB, \"orders\") and texists(SILVER_DB, \"shipments\") and DimProductLK is not None and DimDateLK is not None:\n","    p = DimProductLK.alias(\"p\")\n","    d = DimDateLK.alias(\"d\")\n","\n","    o = (spark.table(f\"{SILVER_DB}.orders\").alias(\"o\")\n","            .select(\"OrderDate\",\"ProductID\",\"QuantityPallets\",\"Source\")\n","            .join(p, on=[\"ProductID\",\"Source\"], how=\"left\")\n","            .withColumn(\"OrderedTons\", F.col(\"o.QuantityPallets\").cast(\"double\") /\n","                                      F.when(F.col(\"p.PalletsToTon\") == 0, F.lit(1.0)).otherwise(F.col(\"p.PalletsToTon\")))\n","            .groupBy(F.to_date(F.col(\"o.OrderDate\")).alias(\"date\"))\n","            .agg(F.sum(\"OrderedTons\").alias(\"OrderedTons\")))\n","\n","    sh = (spark.table(f\"{SILVER_DB}.shipments\").alias(\"s\")\n","            .select(\"Date\",\"ProductID\",\"Pallets\",\"Source\")\n","            .join(p, on=[\"ProductID\",\"Source\"], how=\"left\")\n","            .withColumn(\"ShippedTons\", F.col(\"s.Pallets\").cast(\"double\") /\n","                                       F.when(F.col(\"p.PalletsToTon\") == 0, F.lit(1.0)).otherwise(F.col(\"p.PalletsToTon\")))\n","            .groupBy(F.to_date(F.col(\"s.Date\")).alias(\"date\"))\n","            .agg(F.sum(\"ShippedTons\").alias(\"ShippedTons\")))\n","\n","    fr1 = (o.join(sh, \"date\", \"full\")\n","             .na.fill({\"OrderedTons\":0.0, \"ShippedTons\":0.0})\n","             .withColumn(\"FillRate\", F.when(F.col(\"OrderedTons\") > 0,\n","                                            F.col(\"ShippedTons\")/F.col(\"OrderedTons\"))\n","                                   .otherwise(F.lit(0.0))))\n","    fr2 = fr1.alias(\"fr\").join(d, F.col(\"fr.date\") == F.col(\"d.date\"), how=\"left\")\n","\n","    fr2 = (fr2.withColumn(\"NaturalKey\", F.sha2(F.col(\"fr.date\").cast(\"string\"), 256))\n","               .withColumn(\"SurrogateKey\", sk_hash([\"NaturalKey\"])))\n","\n","    fact_fr = fr2.select(\n","        \"SurrogateKey\",\"NaturalKey\",\n","        \"d.DateSK\",\n","        \"OrderedTons\",\"ShippedTons\",\"FillRate\"\n","    )\n","    merge_upsert(f\"{GOLD_DB}.FactDailyFillRate\", fact_fr, natural_key_cols=[\"NaturalKey\"])\n","\n","# FactWarehouseDailyUtilization (date, warehouse)\n","if texists(SILVER_DB, \"stockmovements\") and texists(SILVER_DB, \"warehouses\") and DimWarehouseLK is not None and DimDateLK is not None:\n","    mv = spark.table(f\"{SILVER_DB}.stockmovements\").select(\"Date\",\"FromType\",\"FromID\",\"ToType\",\"ToID\",\"Pallets\",\"Source\").alias(\"m\")\n","    wh = spark.table(f\"{SILVER_DB}.warehouses\").select(\"WarehouseID\",\"CapacityPallets\",\"Source\").alias(\"w0\")\n","\n","    receipts = (mv.where(F.col(\"m.ToType\") == F.lit(\"Warehouse\"))\n","                  .groupBy(F.col(\"m.Date\").alias(\"Date\"), F.col(\"m.ToID\").alias(\"WarehouseID\"), F.col(\"m.Source\").alias(\"Source\"))\n","                  .agg(F.sum(\"Pallets\").alias(\"receipts_pallets\")))\n","    shipments = (mv.where(F.col(\"m.FromType\") == F.lit(\"Warehouse\"))\n","                   .groupBy(F.col(\"m.Date\").alias(\"Date\"), F.col(\"m.FromID\").alias(\"WarehouseID\"), F.col(\"m.Source\").alias(\"Source\"))\n","                   .agg(F.sum(\"Pallets\").alias(\"shipments_pallets\")))\n","\n","    daily = (receipts.join(shipments, [\"Date\",\"WarehouseID\",\"Source\"], \"full\")\n","                   .na.fill({\"receipts_pallets\":0.0, \"shipments_pallets\":0.0})\n","                   .withColumn(\"delta_pallets\", F.col(\"receipts_pallets\") - F.col(\"shipments_pallets\")))\n","\n","    w = Window.partitionBy(\"WarehouseID\",\"Source\").orderBy(\"Date\").rowsBetween(Window.unboundedPreceding, 0)\n","    eod = (daily.withColumn(\"pallets_occupied\", F.sum(\"delta_pallets\").over(w))\n","               .join(wh, on=[\"WarehouseID\",\"Source\"], how=\"left\")\n","               .withColumn(\"pallets_occupied\", F.when(F.col(\"pallets_occupied\") < 0, F.lit(0.0)).otherwise(F.col(\"pallets_occupied\")))\n","               .withColumn(\"Utilization\", F.when(F.col(\"CapacityPallets\") > 0,\n","                                                 F.col(\"pallets_occupied\")/F.col(\"CapacityPallets\"))\n","                                          .otherwise(F.lit(0.0))))\n","\n","    # Join dims using aliases to avoid ambiguity\n","    d = DimDateLK.alias(\"d\")\n","    wlk = DimWarehouseLK.alias(\"wlk\")\n","    eod = (eod.withColumn(\"EODDate\", F.to_date(F.col(\"Date\")))\n","              .alias(\"x\")\n","              .join(d, F.col(\"x.EODDate\") == F.col(\"d.date\"), how=\"left\")\n","              .join(wlk, on=[\"WarehouseID\",\"Source\"], how=\"left\"))\n","\n","    eod = (eod.withColumn(\"NaturalKey\",\n","                          F.sha2(F.concat_ws(\"|\",\n","                                             F.col(\"x.Date\").cast(\"string\"),\n","                                             F.col(\"x.WarehouseID\").cast(\"string\"),\n","                                             F.coalesce(F.col(\"x.Source\").cast(\"string\"), F.lit(\"\"))), 256))\n","              .withColumn(\"SurrogateKey\", sk_hash([\"NaturalKey\"])))\n","\n","    fact_util = eod.select(\n","        \"SurrogateKey\",\"NaturalKey\",\n","        \"d.DateSK\",\"wlk.WarehouseSK\",\n","        F.col(\"x.Source\").alias(\"Source\"),\n","        F.col(\"pallets_occupied\").alias(\"PalletsOccupied\"),\n","        \"CapacityPallets\",\"Utilization\"\n","    )\n","    merge_upsert(f\"{GOLD_DB}.FactWarehouseDailyUtilization\",\n","                 fact_util,\n","                 natural_key_cols=[\"NaturalKey\"],\n","                 partition_by=[\"Source\"])\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"b0b85492-c77f-4a14-ac16-ec6c7c8853ed","normalized_state":"finished","queued_time":"2025-08-13T15:01:59.9282039Z","session_start_time":"2025-08-13T15:01:59.9300203Z","execution_start_time":"2025-08-13T15:02:11.8352316Z","execution_finish_time":"2025-08-13T15:03:58.0410477Z","parent_msg_id":"f09c2f4d-8991-4010-bca0-cbf3e81f72eb"},"text/plain":"StatementMeta(, b0b85492-c77f-4a14-ac16-ec6c7c8853ed, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["🆕 lakehouse_gd.DimProduct: created\n🆕 lakehouse_gd.DimWarehouse: created\n🆕 lakehouse_gd.DimCustomer: created\n🆕 lakehouse_gd.DimMill: created\n🆕 lakehouse_gd.DimMachine: created\n🆕 lakehouse_gd.DimEmployee: created\n🆕 lakehouse_gd.DimDate: created\n🆕 lakehouse_gd.FactOrders: created\n🆕 lakehouse_gd.FactShipments: created\n🆕 lakehouse_gd.FactDailyFillRate: created\n🆕 lakehouse_gd.FactWarehouseDailyUtilization: created\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc485def-96d3-493d-8550-f9fee702410c"},{"cell_type":"code","source":["%%sql\n","-- =========================================================\n","-- 0) BASIC ROW COUNTS (Gold)\n","-- =========================================================\n","\n","SELECT 'DimProduct'  AS table_name, COUNT(*) AS rows FROM lakehouse_gd.DimProduct  UNION ALL\n","SELECT 'DimWarehouse',               COUNT(*)       FROM lakehouse_gd.DimWarehouse UNION ALL\n","SELECT 'DimCustomer',                COUNT(*)       FROM lakehouse_gd.DimCustomer  UNION ALL\n","SELECT 'DimDate',                    COUNT(*)       FROM lakehouse_gd.DimDate      UNION ALL\n","SELECT 'FactOrders',                 COUNT(*)       FROM lakehouse_gd.FactOrders   UNION ALL\n","SELECT 'FactShipments',              COUNT(*)       FROM lakehouse_gd.FactShipments UNION ALL\n","SELECT 'FactDailyFillRate',          COUNT(*)       FROM lakehouse_gd.FactDailyFillRate UNION ALL\n","SELECT 'FactWarehouseDailyUtilization', COUNT(*)    FROM lakehouse_gd.FactWarehouseDailyUtilization;\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"b0b85492-c77f-4a14-ac16-ec6c7c8853ed","normalized_state":"finished","queued_time":"2025-08-13T15:02:00.2611231Z","session_start_time":null,"execution_start_time":"2025-08-13T15:03:58.0437487Z","execution_finish_time":"2025-08-13T15:04:07.9673913Z","parent_msg_id":"6be2b1a1-790a-4dad-b61e-62f7316c83f0"},"text/plain":"StatementMeta(, b0b85492-c77f-4a14-ac16-ec6c7c8853ed, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":2,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"table_name","type":"string","nullable":false,"metadata":{}},{"name":"rows","type":"long","nullable":false,"metadata":{}}]},"data":[["DimProduct","21"],["DimWarehouse","4"],["DimCustomer","156"],["DimDate","8"],["FactOrders","947"],["FactShipments","1183"],["FactDailyFillRate","7"],["FactWarehouseDailyUtilization","18"]]},"text/plain":"<Spark SQL result set with 8 rows and 2 fields>"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"af6446fe-b63e-4d58-a628-d0afcdc02356"},{"cell_type":"code","source":["%%sql\n","\n","-- =========================================================\n","-- 3) FACTS: FK / REFERENTIAL INTEGRITY CHECKS (SKs present?)\n","-- =========================================================\n","\n","-- Missing ProductSK on Orders\n","SELECT COUNT(*) AS missing_productsk\n","FROM lakehouse_gd.FactOrders fo\n","LEFT ANTI JOIN lakehouse_gd.DimProduct dp\n","ON fo.ProductSK = dp.SurrogateKey;\n","\n","-- Missing DateSK on Orders\n","SELECT COUNT(*) AS missing_datesk\n","FROM lakehouse_gd.FactOrders fo\n","LEFT ANTI JOIN lakehouse_gd.DimDate dd\n","ON fo.DateSK = dd.SurrogateKey;\n","\n","-- Missing WarehouseSK on Shipments\n","SELECT COUNT(*) AS missing_warehousesk\n","FROM lakehouse_gd.FactShipments fs\n","LEFT ANTI JOIN lakehouse_gd.DimWarehouse dw\n","ON fs.WarehouseSK = dw.SurrogateKey;\n","\n","-- Null SK rates (quick view)\n","SELECT \n","  SUM(CASE WHEN DateSK      IS NULL THEN 1 ELSE 0 END) AS null_datesk,\n","  SUM(CASE WHEN ProductSK   IS NULL THEN 1 ELSE 0 END) AS null_productsk,\n","  SUM(CASE WHEN WarehouseSK IS NULL THEN 1 ELSE 0 END) AS null_warehousesk,\n","  SUM(CASE WHEN NaturalKey  IS NULL THEN 1 ELSE 0 END) AS null_customersk\n","FROM lakehouse_gd.FactShipments;  -- change to FactOrders to check that table\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[5,6,7,8],"state":"finished","livy_statement_state":"available","session_id":"b0b85492-c77f-4a14-ac16-ec6c7c8853ed","normalized_state":"finished","queued_time":"2025-08-13T15:04:07.9696192Z","session_start_time":null,"execution_start_time":"2025-08-13T15:04:07.9699767Z","execution_finish_time":"2025-08-13T15:04:13.9775696Z","parent_msg_id":"9b412fc8-704e-4bdf-9550-e755d1835729"},"text/plain":"StatementMeta(, b0b85492-c77f-4a14-ac16-ec6c7c8853ed, 8, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":3,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"missing_productsk","type":"long","nullable":false,"metadata":{}}]},"data":[["0"]]},"text/plain":"<Spark SQL result set with 1 rows and 1 fields>"},"metadata":{}},{"output_type":"execute_result","execution_count":3,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"missing_datesk","type":"long","nullable":false,"metadata":{}}]},"data":[["0"]]},"text/plain":"<Spark SQL result set with 1 rows and 1 fields>"},"metadata":{}},{"output_type":"execute_result","execution_count":3,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"missing_warehousesk","type":"long","nullable":false,"metadata":{}}]},"data":[["0"]]},"text/plain":"<Spark SQL result set with 1 rows and 1 fields>"},"metadata":{}},{"output_type":"execute_result","execution_count":3,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"null_datesk","type":"long","nullable":true,"metadata":{}},{"name":"null_productsk","type":"long","nullable":true,"metadata":{}},{"name":"null_warehousesk","type":"long","nullable":true,"metadata":{}},{"name":"null_customersk","type":"long","nullable":true,"metadata":{}}]},"data":[["0","0","0","0"]]},"text/plain":"<Spark SQL result set with 1 rows and 4 fields>"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"72c5f14c-e6f1-4505-baae-c2dabbaf9104"},{"cell_type":"code","source":["%%sql\n","\n","-- =========================================================\n","-- 5) MEASURE RECONCILIATION vs SILVER (ORDERS)\n","-- =========================================================\n","-- Gold FactOrders vs Silver orders (per day)\n","WITH gold AS (\n","  SELECT fo.DateSK, SUM(fo.OrderedPallets) AS ordered_pallets_gold,\n","         SUM(fo.OrderedTons) AS ordered_tons_gold\n","  FROM lakehouse_gd.FactOrders fo\n","  GROUP BY fo.DateSK\n","),\n","dates AS (\n","  SELECT d.SurrogateKey AS DateSK, d.date\n","  FROM lakehouse_gd.DimDate d\n","),\n","silver AS (\n","  SELECT to_date(o.OrderDate) AS date,\n","         SUM(o.QuantityPallets) AS ordered_pallets_silver,\n","         SUM(CAST(o.QuantityPallets AS DOUBLE) / \n","             CASE WHEN p.PalletsToTon IS NULL OR p.PalletsToTon = 0 THEN 1.0 ELSE p.PalletsToTon END) AS ordered_tons_silver\n","  FROM lakehouse_sv.orders o\n","  JOIN lakehouse_gd.DimProduct p\n","    ON o.ProductID = p.ProductID AND o.Source = p.Source\n","  GROUP BY to_date(o.OrderDate)\n",")\n","SELECT d.date,\n","       g.ordered_pallets_gold, s.ordered_pallets_silver,\n","       g.ordered_tons_gold,    s.ordered_tons_silver,\n","       (g.ordered_pallets_gold - s.ordered_pallets_silver) AS pallets_diff,\n","       (g.ordered_tons_gold - s.ordered_tons_silver)       AS tons_diff\n","FROM gold g\n","JOIN dates d ON g.DateSK = d.DateSK\n","FULL JOIN silver s ON s.date = d.date\n","ORDER BY d.date\n","LIMIT 200;\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"b0b85492-c77f-4a14-ac16-ec6c7c8853ed","normalized_state":"finished","queued_time":"2025-08-13T15:02:00.9405045Z","session_start_time":null,"execution_start_time":"2025-08-13T15:04:13.9794136Z","execution_finish_time":"2025-08-13T15:04:16.3782875Z","parent_msg_id":"d6d70ebc-3fd6-4e3d-ba60-5a05040a4496"},"text/plain":"StatementMeta(, b0b85492-c77f-4a14-ac16-ec6c7c8853ed, 9, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":4,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"date","type":"date","nullable":true,"metadata":{}},{"name":"ordered_pallets_gold","type":"double","nullable":true,"metadata":{}},{"name":"ordered_pallets_silver","type":"long","nullable":true,"metadata":{}},{"name":"ordered_tons_gold","type":"double","nullable":true,"metadata":{}},{"name":"ordered_tons_silver","type":"double","nullable":true,"metadata":{}},{"name":"pallets_diff","type":"double","nullable":true,"metadata":{}},{"name":"tons_diff","type":"double","nullable":true,"metadata":{}}]},"data":[["2025-08-01",8958,"8958",11277.279434941345,11277.279434941345,0,0],["2025-08-02",10966,"10966",13906.976810969827,13906.976810969827,0,0],["2025-08-03",7963,"7963",10027.388368683854,10027.388368683854,0,0],["2025-08-04",6570,"6570",8276.256631969107,8276.256631969107,0,0],["2025-08-05",8845,"8845",11088.855103134385,11088.855103134385,0,0],["2025-08-06",8014,"8014",10101.340980111476,10101.340980111476,0,0],["2025-08-07",9797,"9797",12469.085294904366,12469.085294904366,0,0]]},"text/plain":"<Spark SQL result set with 7 rows and 7 fields>"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"80b4aa4a-bcc8-406f-9601-67bace76d91d"},{"cell_type":"code","source":["%%sql\n","\n","-- =========================================================\n","-- 10) QUICK SPOT CHECKS (top dates / warehouses)\n","-- =========================================================\n","-- Most recent utilization by warehouse\n","SELECT d.date, f.WarehouseSK, f.Source, f.PalletsOccupied, f.CapacityPallets, f.Utilization\n","FROM lakehouse_gd.FactWarehouseDailyUtilization f\n","JOIN lakehouse_gd.DimDate d ON f.DateSK = d.SurrogateKey\n","ORDER BY d.date DESC, f.WarehouseSK\n","LIMIT 100;\n","\n","-- Daily totals orders vs shipments in Gold (pallets)\n","SELECT d.date,\n","       SUM(o.OrderedPallets) AS orders_pallets,\n","       SUM(s.ShippedPallets) AS shipments_pallets\n","FROM lakehouse_gd.DimDate d\n","LEFT JOIN lakehouse_gd.FactOrders o     ON o.DateSK = d.SurrogateKey\n","LEFT JOIN lakehouse_gd.FactShipments s  ON s.DateSK = d.SurrogateKey\n","GROUP BY d.date\n","ORDER BY d.date DESC\n","LIMIT 200;\n"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[10,11],"state":"finished","livy_statement_state":"available","session_id":"b0b85492-c77f-4a14-ac16-ec6c7c8853ed","normalized_state":"finished","queued_time":"2025-08-13T15:04:16.3804342Z","session_start_time":null,"execution_start_time":"2025-08-13T15:04:16.3807746Z","execution_finish_time":"2025-08-13T15:04:21.2652799Z","parent_msg_id":"ea0325ae-7821-466f-b0f0-1863ecbc2551"},"text/plain":"StatementMeta(, b0b85492-c77f-4a14-ac16-ec6c7c8853ed, 11, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":5,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"date","type":"date","nullable":true,"metadata":{}},{"name":"WarehouseSK","type":"long","nullable":true,"metadata":{}},{"name":"Source","type":"string","nullable":true,"metadata":{}},{"name":"PalletsOccupied","type":"double","nullable":true,"metadata":{}},{"name":"CapacityPallets","type":"integer","nullable":true,"metadata":{}},{"name":"Utilization","type":"double","nullable":true,"metadata":{}}]},"data":[["2025-08-07","88274830626195043","USA",23615,50000,0.4723],["2025-08-07","6204754575077654592","Brazil",22366,40000,0.55915],["2025-08-07","8062166303583598165","Sweden",20832,38000,0.5482105263157895],["2025-08-06","88274830626195043","USA",19550,50000,0.391],["2025-08-06","6204754575077654592","Brazil",21181,40000,0.529525],["2025-08-06","8062166303583598165","Sweden",20262,38000,0.5332105263157895],["2025-08-05","88274830626195043","USA",16310,50000,0.3262],["2025-08-05","6204754575077654592","Brazil",15271,40000,0.381775],["2025-08-05","8062166303583598165","Sweden",15159,38000,0.39892105263157895],["2025-08-04","88274830626195043","USA",15108,50000,0.30216],["2025-08-04","6204754575077654592","Brazil",14275,40000,0.356875],["2025-08-04","8062166303583598165","Sweden",11902,38000,0.3132105263157895],["2025-08-03","88274830626195043","USA",8599,50000,0.17198],["2025-08-03","6204754575077654592","Brazil",9207,40000,0.230175],["2025-08-03","8062166303583598165","Sweden",9605,38000,0.25276315789473686],["2025-08-02","88274830626195043","USA",7766,50000,0.15532],["2025-08-02","6204754575077654592","Brazil",7302,40000,0.18255],["2025-08-02","8062166303583598165","Sweden",5510,38000,0.145]]},"text/plain":"<Spark SQL result set with 18 rows and 6 fields>"},"metadata":{}},{"output_type":"execute_result","execution_count":5,"data":{"application/vnd.synapse.sparksql-result+json":{"schema":{"type":"struct","fields":[{"name":"date","type":"date","nullable":true,"metadata":{}},{"name":"orders_pallets","type":"double","nullable":true,"metadata":{}},{"name":"shipments_pallets","type":"double","nullable":true,"metadata":{}}]},"data":[["2025-08-08",null,null],["2025-08-07",1273610,1045688],["2025-08-06",1739038,1666176],["2025-08-05",1194075,1103531],["2025-08-04",1307430,1303029],["2025-08-03",963523,939520],["2025-08-02",3432358,3200668],["2025-08-01",609144,359700]]},"text/plain":"<Spark SQL result set with 8 rows and 3 fields>"},"metadata":{}}],"execution_count":5,"metadata":{"microsoft":{"language":"sparksql","language_group":"synapse_pyspark"},"collapsed":false},"id":"f77c78ff-1a20-4d31-8392-91551bcf902b"},{"cell_type":"code","source":["from notebookutils import mssparkutils\n","\n","delete_tables = False  # <-- set True to delete the folders too\n","\n","if delete_tables:\n","    # same list you used when creating\n","    tables = [\n","        \"DimCustomer\",\"DimEmployee\",\"DimMachine\",\"DimMill\",\"DimProduct\",\"DimWarehouse\",\"DimDate\",\n","        \"FactDailyFillRate\",\n","        \"FactOrders\",\"FactWarehouseDailyUtilization\",\"FactShipments\"\n","    ]\n","\n","    # Drop from the current Lakehouse database (what Fabric shows as the schema)\n","    db = spark.catalog.currentDatabase()\n","    print(f\"Dropping from database: {db}\")\n","\n","    for t in tables:\n","        spark.sql(f\"DROP TABLE IF EXISTS {db}.{t.lower()}\")\n","        print(f\"🗑️  Dropped: {db}.{t}\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"b0b85492-c77f-4a14-ac16-ec6c7c8853ed","normalized_state":"finished","queued_time":"2025-08-13T15:02:01.5350385Z","session_start_time":null,"execution_start_time":"2025-08-13T15:04:21.2675316Z","execution_finish_time":"2025-08-13T15:04:21.5818456Z","parent_msg_id":"b5af3803-ef77-4eb0-bb66-8988bbfd2838"},"text/plain":"StatementMeta(, b0b85492-c77f-4a14-ac16-ec6c7c8853ed, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"98598425-99c8-4705-838d-c11c79380f07"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"synapse_widget":{"version":"0.1","state":{}},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"default_lakehouse":"fc74514f-4add-4b17-bdf6-5fecae750cf3","known_lakehouses":[{"id":"fc74514f-4add-4b17-bdf6-5fecae750cf3"},{"id":"6d7f2b2f-8940-484b-901d-de2677b03f1f"}],"default_lakehouse_name":"Lakehouse_Gd","default_lakehouse_workspace_id":"399eee70-3af7-4903-a62d-6c5acb3ade34"}}},"nbformat":4,"nbformat_minor":5}